# Ollama é…ç½®å®Œæˆ âœ…

æ­å–œï¼æ‚¨å·²æˆåŠŸé…ç½® Dify ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ã€‚

## å¿«é€Ÿå¼€å§‹

### 1. éªŒè¯é…ç½®

```bash
cd /workspaces/dify.git
uv run --project api python scripts/stress-test/test_ollama_quick.py
```

åº”è¯¥çœ‹åˆ°ï¼š
```
âœ… OLLAMA IS READY!
```

### 2. å¯åŠ¨ Dify

```bash
./scripts/stress-test/start_dify_with_ollama.sh
```

æˆ–æ‰‹åŠ¨å¯åŠ¨ï¼š
```bash
cd /workspaces/dify.git/api
uv run gunicorn --bind 0.0.0.0:5001 --workers 4 --worker-class gevent app:app
```

### 3. é…ç½® Web UI

1. æ‰“å¼€ `http://localhost:3000`
2. è¿›å…¥ **Settings** â†’ **Model Provider** â†’ **Ollama**
3. é…ç½®ï¼š
   - **Base URL**: `http://172.17.0.1:11434`
   - **API Key**: `ollama` (ä»»æ„å€¼)
4. ç‚¹å‡» **Save**

### 4. åˆ›å»ºå·¥ä½œæµ

1. åˆ›å»ºæ–°çš„ Chatflow/Workflow
2. æ·»åŠ  LLM èŠ‚ç‚¹
3. é€‰æ‹© **Ollama** æä¾›å•†
4. é€‰æ‹©æ¨¡å‹ï¼š`deepseek-r1:latest`
5. å¼€å§‹ä½¿ç”¨ï¼

## é…ç½®è¯¦æƒ…

- **Ollama Host**: `http://172.17.0.1:11434`
- **å½“å‰æ¨¡å‹**: `deepseek-r1:latest`
- **é…ç½®æ–‡ä»¶**: `/workspaces/dify.git/api/.env`

## æ·»åŠ æ›´å¤šæ¨¡å‹

åœ¨å®¿ä¸»æœºä¸Šè¿è¡Œï¼š

```bash
# å¿«é€Ÿè½»é‡çº§æ¨¡å‹
ollama pull qwen2.5:3b
ollama pull llama3.2:3b

# æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹
ollama list
```

## æ•…éšœæ’æŸ¥

### è¿æ¥é—®é¢˜

ç¡®ä¿ Ollama åœ¨å®¿ä¸»æœºä¸Šç›‘å¬æ‰€æœ‰æ¥å£ï¼š

```bash
# åœ¨å®¿ä¸»æœºæ‰§è¡Œ
export OLLAMA_HOST=0.0.0.0:11434
ollama serve
```

éªŒè¯è¿æ¥ï¼š
```bash
# åœ¨ Dev Container ä¸­
curl http://172.17.0.1:11434/api/tags
```

## æœ‰ç”¨çš„è„šæœ¬

- **å¿«é€Ÿæµ‹è¯•**: `scripts/stress-test/test_ollama_quick.py`
- **å®Œæ•´æµ‹è¯•**: `scripts/stress-test/test_ollama_integration.py`
- **é…ç½®è„šæœ¬**: `scripts/stress-test/setup/configure_ollama.py`
- **å¯åŠ¨è„šæœ¬**: `scripts/stress-test/start_dify_with_ollama.sh`

## è¯¦ç»†æ–‡æ¡£

æŸ¥çœ‹å®Œæ•´è®¾ç½®æŒ‡å—ï¼š
```bash
cat scripts/stress-test/OLLAMA_SETUP.md
```

## é—®é¢˜æ’æŸ¥

è¿è¡Œè¯Šæ–­ï¼š
```bash
uv run --project api python scripts/stress-test/test_ollama_quick.py
```

---

**äº«å—ä½¿ç”¨å®Œå…¨æœ¬åœ°åŒ–çš„ LLMï¼** ğŸ‰
