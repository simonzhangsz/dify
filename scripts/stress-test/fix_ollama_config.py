"""
ä¿®å¤ Ollama æ¨¡å‹é…ç½®é—®é¢˜
"""
import requests
import json


def fix_ollama_config():
    """ä¿®å¤ Ollama é…ç½®ï¼Œä½¿ç”¨å®é™…å­˜åœ¨çš„æ¨¡å‹"""
    
    ollama_host = "http://172.17.0.1:11434"
    
    print("=" * 60)
    print("OLLAMA é…ç½®ä¿®å¤å·¥å…·")
    print("=" * 60)
    print()
    
    # 1. è·å–å®é™…å¯ç”¨çš„æ¨¡å‹
    print("ğŸ” æ£€æŸ¥ Ollama ä¸­çš„æ¨¡å‹...")
    try:
        response = requests.get(f"{ollama_host}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            if not models:
                print("âŒ Ollama ä¸­æ²¡æœ‰æ¨¡å‹ï¼")
                print("\nğŸ’¡ è¯·åœ¨å®¿ä¸»æœºä¸Šæ‹‰å–æ¨¡å‹ï¼š")
                print("   ollama pull qwen2.5:3b     # æ¨èï¼šå¿«é€Ÿä¸­æ–‡æ¨¡å‹")
                print("   ollama pull llama3.2:3b    # æ¨èï¼šå¿«é€Ÿè‹±æ–‡æ¨¡å‹")
                print("   ollama pull deepseek-r1    # æ‚¨å½“å‰çš„æ¨¡å‹")
                return
            
            print(f"âœ… æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹ï¼š")
            for i, model in enumerate(models, 1):
                print(f"   {i}. {model['name']}")
                print(f"      å¤§å°: {model['size'] / (1024**3):.1f} GB")
                print(f"      å®¶æ—: {model['details'].get('family', 'N/A')}")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹è¿›è¡ŒéªŒè¯
            test_model = models[0]['name']
            print(f"\nğŸ§ª ä½¿ç”¨ '{test_model}' è¿›è¡Œè¿æ¥æµ‹è¯•...")
            
            # æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸçš„å¯ç”¨
            test_response = requests.post(
                f"{ollama_host}/api/generate",
                json={
                    'model': test_model,
                    'prompt': 'test',
                    'stream': False,
                    'options': {
                        'num_predict': 1  # åªç”Ÿæˆ 1 ä¸ª token
                    }
                },
                timeout=30
            )
            
            if test_response.status_code == 200:
                print(f"âœ… æ¨¡å‹ '{test_model}' å¯ä»¥æ­£å¸¸ä½¿ç”¨")
            else:
                print(f"âš ï¸  æ¨¡å‹æµ‹è¯•è¿”å›çŠ¶æ€ç : {test_response.status_code}")
            
            print("\n" + "=" * 60)
            print("ğŸ“ åœ¨ Dify Web UI ä¸­é…ç½® Ollama")
            print("=" * 60)
            print("\næ­¥éª¤ï¼š")
            print("1. æ‰“å¼€ http://localhost:3000")
            print("2. è¿›å…¥ Settings â†’ Model Provider")
            print("3. æ‰¾åˆ° Ollama éƒ¨åˆ†")
            print("4. é…ç½®ä»¥ä¸‹ä¿¡æ¯ï¼š")
            print(f"   Base URL: {ollama_host}")
            print("   API Key: ollama  (ä»»æ„å€¼)")
            print("\n5. âš ï¸  é‡è¦ï¼šå¦‚æœå‡ºç° 'æ¨¡å‹æœªæ‰¾åˆ°' é”™è¯¯")
            print("   è¿™æ˜¯ Dify çš„éªŒè¯æœºåˆ¶é—®é¢˜ï¼Œæœ‰ä¸¤ä¸ªè§£å†³æ–¹æ¡ˆï¼š")
            print()
            print("   æ–¹æ¡ˆ A - æ‹‰å–å¸¸è§æ¨¡å‹ï¼ˆæ¨èï¼‰ï¼š")
            print("   åœ¨å®¿ä¸»æœºæ‰§è¡Œï¼š")
            print("   ollama pull qwen2.5:3b")
            print("   æˆ–")
            print("   ollama pull llama3.2:3b")
            print()
            print("   æ–¹æ¡ˆ B - ç›´æ¥ä¿å­˜é…ç½®ï¼š")
            print("   æœ‰äº› Dify ç‰ˆæœ¬ä¼šåœ¨ä¿å­˜æ—¶éªŒè¯é»˜è®¤æ¨¡å‹")
            print("   å³ä½¿éªŒè¯å¤±è´¥ï¼Œé…ç½®å¯èƒ½å·²ç»ä¿å­˜")
            print("   å°è¯•åœ¨å·¥ä½œæµä¸­ç›´æ¥é€‰æ‹©æ‚¨å·²æœ‰çš„æ¨¡å‹")
            print()
            print(f"6. åˆ›å»ºå·¥ä½œæµæ—¶ï¼Œé€‰æ‹©æ¨¡å‹ï¼š{test_model}")
            
            # æä¾›ä¸€é”®æ‹‰å–è„šæœ¬
            print("\n" + "=" * 60)
            print("ğŸš€ å¿«é€Ÿè§£å†³æ–¹æ¡ˆ")
            print("=" * 60)
            print("\nåœ¨å®¿ä¸»æœºä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ‹‰å–è½»é‡çº§æ¨¡å‹ï¼š")
            print()
            print("# é€‰é¡¹ 1: ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼ˆæ¨èï¼‰")
            print("ollama pull qwen2.5:3b")
            print()
            print("# é€‰é¡¹ 2: è‹±æ–‡ä¼˜åŒ–æ¨¡å‹")
            print("ollama pull llama3.2:3b")
            print()
            print("# é€‰é¡¹ 3: å¾®è½¯å°æ¨¡å‹")
            print("ollama pull phi3:mini")
            print()
            print("æ‹‰å–åï¼ŒDify çš„éªŒè¯å°±ä¼šé€šè¿‡ï¼")
            
        else:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama: {response.status_code}")
            
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶")
        print("\nè¯·ç¡®ä¿ï¼š")
        print("1. Ollama åœ¨å®¿ä¸»æœºè¿è¡Œ")
        print("2. Ollama ç›‘å¬ 0.0.0.0:11434")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


def show_current_models():
    """æ˜¾ç¤ºå½“å‰æ¨¡å‹å’Œæ¨èçš„æ“ä½œ"""
    ollama_host = "http://172.17.0.1:11434"
    
    try:
        response = requests.get(f"{ollama_host}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            
            print("\n" + "=" * 60)
            print("ğŸ“Š æ¨¡å‹åˆ†æ")
            print("=" * 60)
            
            if models:
                print(f"\nå½“å‰å·²å®‰è£… {len(models)} ä¸ªæ¨¡å‹ï¼š")
                for model in models:
                    name = model['name']
                    size_gb = model['size'] / (1024**3)
                    family = model['details'].get('family', 'unknown')
                    
                    # åˆ¤æ–­æ¨¡å‹ç±»å‹
                    if 'deepseek-r1' in name.lower():
                        model_type = "æ¨ç†æ¨¡å‹ï¼ˆè¾ƒæ…¢ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡ï¼‰"
                    elif 'qwen' in name.lower():
                        model_type = "ä¸­æ–‡ä¼˜åŒ–"
                    elif 'llama' in name.lower():
                        model_type = "é€šç”¨æ¨¡å‹"
                    else:
                        model_type = "å…¶ä»–"
                    
                    print(f"\nâœ“ {name}")
                    print(f"  ç±»å‹: {model_type}")
                    print(f"  å¤§å°: {size_gb:.1f} GB")
                    print(f"  å®¶æ—: {family}")
                
                print("\nğŸ’¡ å»ºè®®ï¼š")
                if len(models) == 1 and 'deepseek-r1' in models[0]['name'].lower():
                    print("æ‚¨åªæœ‰ DeepSeek-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†æ¨¡å‹ï¼Œå“åº”å¯èƒ½è¾ƒæ…¢ã€‚")
                    print("å»ºè®®æ·»åŠ ä¸€ä¸ªå¿«é€Ÿæ¨¡å‹ç”¨äºæ—¥å¸¸å¯¹è¯ï¼š")
                    print()
                    print("åœ¨å®¿ä¸»æœºæ‰§è¡Œï¼š")
                    print("ollama pull qwen2.5:3b    # 1.9GB, å¿«é€Ÿä¸­æ–‡")
                    print("ollama pull llama3.2:3b   # 2.0GB, å¿«é€Ÿè‹±æ–‡")
            else:
                print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹")
                
    except Exception as e:
        print(f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {e}")


if __name__ == '__main__':
    fix_ollama_config()
    show_current_models()
